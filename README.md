

<p align="center">
  <img width="550" height="400" src="https://raw.githubusercontent.com/carlodavid012/100-Days-Of-ML-Code/master/img/100-days-of-code-challenge-accepted.jpg?token=Ab-JR2SPmA7cCsshDhhso-QKmTBJEI_Qks5cjHL1wA%3D%3D">
</p>

This repository contains my progress for the challenge 100DaysOfMLCode, It is a committment to better my understanding of this powerful tool by dedicating at least 1 hour of my time everyday to studying and/or coding machine learning and deep learning for 100 days.



## Day 0 : July 6, 2018

**Today's Progress** : I have setup all the things I needed to complete this challenge. I completed the first part of Introduction to Data Science in Python on Coursera to learn python fundamentals and use advanced Python features, including lambdas, list comprehensions and the numpy library.

**Thoughts** : Hope this will be exciting. The course is really good and it contain lots of good contents. It is good for a beginner.

**Link of work**: [Introduction to Data Science with Python](https://www.coursera.org/learn/python-data-analysis)

## Day 1 : July 7, 2018

**Today's Progress** : Finished the first part of the Machine Learning course on coursera by Andrew Ng up to Linear Regression with One Variable.

**Thoughts** : I learned the application of Linear Regression with one variable to housing price prediction, the notion of a cost function, and the gradient descent method for learning.

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 2 : July 8, 2018

**Today's Progress** : Since Linear Algebra is necessary on learning Machine Learning, I review on basic linear algebra concepts. I finished the Linear Algebra Review on machine learning course on Coursera. 

**Thoughts** : I learned about matrices and vectors, addition and scalar multiplication, matrix-vector and matrix-matrix multiplication, matrix multiplication properties, matrix inverse and matrix transpose operation.

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 3 : July 9, 2018

**Today's Progress** : Learned linear regression using gradient descent and its equations.

**Thoughts** : The concepts are really good. The Math is explained in a practical way.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/Day%203%20-%20linear%20regression%20using%20gradient%20descent/gradient_descent.py) | [How to Do Linear Regression using Gradient Descent](https://www.youtube.com/watch?v=XdM6ER7zTLk)

## Day 4 : July 10, 2018

**Today's Progress** : Learned about Linear Regression with multiple variables, Polynomial Regression, Feature Scaling, and Normal Equation. I  also learned using Octave and did some basic operations, plotting data, control statements, vectorization and finished the first programming assignment.

**Thoughts** : Some concepts are quite difficult at first but I'm able to understand it. I learned how linear regression can be extended to accommodate multiple input features and best practices for implementing linear regression.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/commit/c43391cb2ec8128f548e15dc102357c25b9645c7) | [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 5 : July 11, 2018

**Today's Progress** : Learned about Logistic Regression for classification. I also learned about the notion of classification, the cost function for logistic regression, and the application of logistic regression to multi-class classification.

**Thoughts** : There's a lot of new concepts I learned in this topic. One of it is solving the problem of overfitting using regularization. 

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 6 : July 12, 2018

**Today's Progress** : Watched the Logistic Regression video on youtube by Siraj. I learned about logistic regression using Newton's method for optimization.

**Thoughts** : There's a lot of new terms that I encountered so I need to review them. One of the things I learned is Newton's method usually converges faster than gradient descent when maximizing logistic regression log likelihood.

**Link of work**: [Logistic Regression by Siraj](https://www.youtube.com/watch?v=D8alok2P468&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D&index=4)

## Day 7 : July 13, 2018

**Today's Progress** : Finished the programming assignment for Logistic Regression in the Machine Learning Course on Coursera.

**Thoughts** : The assignment is challenging and difficult for me because I need to write the solution using Octave. The programming exercise tutorials and discussion forums of the course helped me in order to finish it.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/commit/47b70f0fa40c907ff413193013ff0537035663c5)

## Day 8 : July 14, 2018

**Today's Progress** : Learned about how neural networks works and how it learns using backpropagation. 

**Thoughts** : Artificial Neural Networks are pretty amazing.

**Link of work**: [How do Neural Networks work](https://www.youtube.com/watch?v=r9vQhRyrT1M) | [How do Neural Networks Learn](https://www.youtube.com/watch?v=KlKClvoHHoI) | [Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

## Day 9 : July 15, 2018

**Today's Progress** : I decided to go through Andrew Trask's A Neural Network in 11 lines of Python to really learn how every line worked, and it's been very helpful. 

**Thoughts** : I had to review some matrix math and look up some numpy function calls that he uses, but it was worth it.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/commit/56a10f245fd9e9391f62dc84f151a4ac45168525) | [Neural Networks in Python Part 1](https://iamtrask.github.io/2015/07/12/basic-python-network/)

## Day 10 : July 16, 2018

**Today's Progress** : Watched a video from Siraj Raval on how to make a neural network in python from scratch.

**Thoughts** : Learned more about neural networks and see how it is implemented in python code.

**Link of work**: [Neural Networks from Scratch](https://www.youtube.com/watch?v=vcZub77WvFA)

## Day 11 : July 17, 2018

**Today's Progress** : Continued my machine learning course on coursera which is on Neural Networks. 

**Thoughts** : I learned more theory and intuition about neural networks.

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 12 : July 18, 2018

**Today's Progress** : Learned more about using pandas, which is a Python library for data cleaning and processing. I also learned how to read in data into DataFrame structures, how to query these structures, and the details about such structures are indexed. 

**Thoughts** : Pandas is a very powerful library. It allows you to perform complicated operations efficiently with a small amount of code.

**Link of work**: [Introduction to Data Science with Python](https://www.coursera.org/learn/python-data-analysis)

## Day 13 : July 19, 2018

**Today's Progress** : Continued my Machine Learning course on coursera which is on Week 4 (Neural Networks).

**Thoughts** : Learned about application of neural networks and its examples.

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 14 : July 20, 2018

**Today's Progress** : Today, I decided to brush up my knowledge on Linear Algebra. I watched 3blue1brown video on youtube and finished up to matrix multiplication. 

**Thoughts** : Having a good knowledge on linear algebra is important in learning ML.

**Link of work**: [Essence of Linear Algebra](https://www.youtube.com/watch?v=kjBOesZCoqc&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)

## Day 15 : July 21, 2018

**Today's Progress** : Continued learning linear algebra. Finished up to dot products.

**Thoughts** : Having a good knowledge on linear algebra is important in learning ML.

**Link of work**: [Essence of Linear Algebra](https://www.youtube.com/watch?v=kjBOesZCoqc&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)

## Day 16 : July 22, 2018

**Today's Progress** : Today, I watched the Google machine learning crash course up to validation part.

**Thoughts** : The course has a lot of good content and it is good for beginners.

**Link of work**: [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)

## Day 17 : July 23, 2018

**Today's Progress** : Continued google machine learning crash course up to regularization.

**Thoughts** : Learned about representation,good habits in selecting features, and feature crosses. I also learned about regularization which is used to avoid overfitting.

**Link of work**: [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)

## Day 18 : July 24, 2018

**Today's Progress** : Continued google machine learning crash course up to Introduction to neural nets. I learned about classification, confusion matrix and neural networks.

**Thoughts** : Regularization is important in logistic regression modeling. Accuracy alone doesn't tell the full story when you're working in an imbalanced dataset.

**Link of work**: [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)

## Day 19 : July 25, 2018

**Today's Progress** : Continued google machine learning crash course up to multi-class neural nets.

**Thoughts** : Learned about multi-class neural networks using softmax and one vs all.

**Link of work**: [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)

## Day 20 : July 26, 2018

**Today's Progress** : Learned about support vector machines which can be used for both classification and regression problems.

**Thoughts** : Support vector machines are great when you have small dataset. The decision of which classifier to choose depends on the dataset and complexity of the problem.

**Link of work**: [Support Vector Machines](https://www.youtube.com/watch?v=g8D5YL6cOSE&index=2&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D)

## Day 21 : July 27, 2018

**Today's Progress** : Learned about k-means clustering which is a type of unsupervised learning algorithm.

**Thoughts** : K-means is usually used when your data is numeric because it doesn't work with categorical features. If you how many classes you want to find, use that as your value for k. If not, you can use the elbow method to find the number of clusters you want to find.

**Link of work**: [K-Means Clustering](https://www.youtube.com/watch?v=9991JlKnFmk&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D&index=6)

## Day 22 : July 28, 2018

**Today's Progress** : Learned about convolutional neural networks which is used for image classification.

**Thoughts** : CNN is quite difficult for me, there's a lot concept that I need to review again.

**Link of work**: [Convolutional Neural Networks](https://www.youtube.com/watch?v=FTr3n7uBIuE&index=8&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D)

## Day 23 : November 29, 2018

So, I decided to continue writing my progress on learning ML and DL. Luckily, I'm selected as one of the recipient of Udacity Pytorch Scholarship Challenge. Two weeks have already passed since the beginning of the challenge, from now on I will start writing my journey on learning Deep Learning. You can also take the course for free [Intro to Deep Learning with Pytorch](https://www.udacity.com/course/deep-learning-pytorch--ud188)

**Link of work**: [Pytorch challenge Scholarship Notes](https://github.com/carlodavid012/pytorch_challenge)

## Day 24 : November 30, 2018

**Today's Progress** : Back at it again! Today, I review some concepts of neural networks and study some basics of pytorch and create a simple neural network using pytorch from scratch.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/notebooks/intro_to_neural_nets.ipynb)

## Day 25 : December 1, 2018

**Today's Progress** : Today, I  build a neural network that can predict the digit in the image using the MNIST dataset. We can build one of these deep networks using only weight matrices as I did in the previous notebook, but in general it's very cumbersome and difficult to implement. PyTorch has a nice module nn that provides a nice way to efficiently build large neural networks.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/notebooks/neural_networks.ipynb)

## Day 26 : December 2, 2018

**Today's Progress** : I continued the notebook on classifying digits using MNIST dataset. So far, i've been using softmax activation function but in general any function can be used as activation function, but it must be non linear, here are some examples of activation function: sigmoid, Tanh(hyperbolic tangent) and ReLU (rectified linear unit). In practice, ReLU is usually used as activation function in hidden layers.


**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/notebooks/neural_networks.ipynb)

## Day 27 : December 3, 2018

**Today's Progress** : I haven't write code today, but I watched some videos. To review, Loss function evaluates how bad our model performs or how far we are from the actual value. To minimize the loss, we do a process called gradient descent which is used to update our weights. For single layer networks, gradient descent is straightforward to implement. Training multilayer networks is done through backpropagation which is really just an application of the chain rule from calculus. Through the nn module, PyTorch provides losses such as the cross-entropy loss (nn.CrossEntropyLoss), it also provides a module, autograd, for automatically calculating the gradients of tensors

**Link of course**: [Intro to Deep Learning with Pytorch](https://www.udacity.com/course/deep-learning-pytorch--ud188)


## Day 28 : December 4, 2018

**Today's Progress** : Today I read about regularization, it means we add a penalty on the different parameters of the model to reduce the freedom of the model. Hence, the model will be less likely to fit the noise of the training data and will improve the generalization abilities of the model. In other words, regularization discourages learning a more complex or flexible model, so as to avoid the risk of overfitting. There are two types of regularization, the L1 regularization adds a penalty equal to the sum of the absolute value of the coefficients, which will shrink some parameters to zero. The L2 regularization adds a penalty equal to the sum of the squared value of the coefficients, it will force the parameters to be relatively small, the bigger the penalization, the smaller (and the more robust) the coefficients are.

**Link of article**: [Regularization](https://www.kdnuggets.com/2018/01/regularization-machine-learning.html)

## Day 29 : December 9, 2018

**Today's Progress** : Today I learned about using nn.Sequential in pytorch which is a convenient way to build networks where a tensor is passed sequentially through operations, and transforms, which is used to preprocess our data such as Normalize, Rescale, RandomCrop and chain it together using Compose

## Day 30 : March 2, 2019

**Today's Progress** : It's been a long time since my last update,I should have finished it by now :( I've been awarded a scholarship for the [Deep Learning Nanodegree](https://www.udacity.com/course/deep-learning-nanodegree--nd101) last January 18, so I decided to continue write my progress on learning DL. There will be five projects that I will build in this course:

~~● Predicting Bike-Sharing Patterns~~

● Dog Breed Classifier

● Generate TV Scripts

● Generate Faces

● Deploy a Sentiment Analysis Model

I already finished the first project, so I decided to review the previous lessons. Today, I reviewed on saving and loading models in pytorch, how to load images for training using ImageFolder, and how to peform data augmentation to introduce randomness in input data using transforms.

**Link of work**: [Predicting Bike-Sharing Patterns](https://github.com/carlodavid012/Predicting-Bike-Sharing-Patterns)

## Day 31 : March 3, 2019

**Today's Progress** : Today I reviewed the lessons on transfer learning using PyTorch. Basically, transfer learning means using the learning of a previous model and reuse it on different task. I used a pretrained model called ResNet to classify dogs and cats images. 

**Link of work**: [Transfer learning in pytorch](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/notebooks/dogs_and_cats.ipynb)

## Day 32 : March 4, 2019

**Today's Progress** : Today, I just watched lectures on Convolutional Neural Networks  which is useful for image recognition because it can capture spatial information, I also learned about applications of CNN such as image classification and text classification.

## Day 33 : March 5, 2019

**Today's Progress** : Today I review on classifying MNIST dataset using multilayer perceptron, I also learned about data normalization in PyTorch which is an important preprocessing step because it ensures that each image input comes from a standard distribution, in other words, one input image is in the same range as another image. 

## Day 34 : March 6, 2019

**Today's Progress** : The exact number of epochs to train a network that is accurate and not overfitting is hard to determine, one method to do it is to split the data into three sets, training set, validation set and test set. We use the training set to find the patterns and update the weights, and use validation set to check how the model generalizes in the validation set. After each epoch, we look at the training loss and validation loss, if the training loss is decreasing but the validation loss is increasing then the model is overfitting, and we need to stop training when the validation loss starts to increase.

## Day 35 : March 7, 2019

**Today's Progress** : Today, I finished the exercise on training an Multilayer perceptron(MLP) to classify images from MNIST handwritten digit database.

**Link of work**: [Multilayer Perceptron MNIST](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/notebooks/mlp_mnist.ipynb)

## Day 36 : March 8, 2019

**Today's Progress** : I added a Validation set to the MLP Mnist code. We use validation set to check how our model generalizes during training, it also tells us when to stop training the model(when the validation loss stops decreasing, especially when the validation loss starts increasing and tra training loss is still decreasing).

**Link of work**: [Multilayer Perceptron MNIST with Validation Set](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/notebooks/mlp_mnist_with_validation.ipynb)

## Day 37 : March 9, 2019

**Today's Progress** : Previously, I was using MLP to classifiy handwritten digits. But for complex image recognition problems, its better to use Convolutional Neural networks. In MLP, we always use fully connected layers, so in a 28 x 28 pixels of hand written image, we already used over half million of parameters, so imagine if we have more complex images then we will need million of parameters. Another problem is that, we throw away the 2D information in an image when we flatten the matrix into a vector. CNN's can address these problems. First, we only used sparsely connected layers. Also it accepts matrix as inputs, so we don't throw away the 2D information in an image. This spatial information or knowledege of where the pixels are connected 
in reference to each other is relevant in understanding the image.

## Day 38 : March 10, 2019

**Today's Progress** : Today, I learned about filters in CNN, these filters produce an output that shows edges of objects and different textures.I also learned about frequency in images, we have high frequency in an image when the intensity changes a lot, level of brightness changes quickly from one pixel to the next. We have low frequency when the level of brightness changes very slowly. High frequency images corresponds to the edges of objects in an image, which can help us classify those objects.

## Day 39 : March 11, 2019

**Today's Progress** : I learned about high-pass filters, which is used to sharpen an image, and enhance high frequency part of an image. I also learned about convolution kernels,  which is just a matrix of numbers that modifies an image. Important note is that for edge detection, all elements must sum to 0 because this filter is computing the difference or change between its neighbor pixels. If these kernels did not add up to 0, it means it is postively or negatively weighted which will have the effect of brightening or darkening the entired filtered image. To apply this filter, an input image: F(x,y) is convolved with a kernel K, and convolution is represented by asterisk. `K * F(x,y) = output image` 

## Day 40: March 12, 2019

**Today's Progress**: Today I learned about convolutional layers, where it can learned spatial information, and extract features and detect edges. It is produced by applying series of many convolutional kernels, in other words, it is just a stack of feature maps, which is just the output of the filtered image. Convolutional layers are locally connected as opposed to fully connected layers, and it has also weight sharing. The CNN structure is: Input image -> Convolutional Layer -> Pooling Layer -> Full connected layer -> Class prediction.

## Day 41: March 13, 2019

**Today's Progress**:Today I learned about stride, which is just the amount or step size by which the filter slides over the image, and padding which means adding zeros to give the filter more space to move across the image.
Another layer in CNN is the Pooling layer, which take convolutional layer as input. The purpose of pooling layer is to reduce the dimensionality of convolutional layer. There are two types of pooling layers: Maxpooling and Average pooling. Max pooling is often used because its better at noticing the most important features and edges of an image. 
A convolutional layer + activation function, followed by a pooling layer, and a linear layer (to create a desired output size) make up the basic layers of a CNN.

## Day 42: March 14, 2019

**Today's Progress**: So, I finished the first part on Convolutional Neural Networks lessons. In the last notebook, I trained a CNN model to classify images on CIFAR-10 database. Next, I will learn more details about Transfer Learning.

**Link of work**: [CNN on CIFAR-10 Database](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/notebooks/Convolutional_neural_networks.ipynb)

## Day 43: March 15, 2019

**Today's Progress**: To review, Transfer Learning is using a pre-trained Network to a new different dataset, instead of training a CNN from scratch, we use the knowledge of a pretrained Network to classify the image in the dataset. There are different strategies in using Transfer Learning. Say for example we use the VGG Network. In the first filters in the convolutional layers it can classify general features like edges and shapes, so we only need to remove the last layer that is specific and replace it with our new fully connected layer. This technique only works if our dataset is small and similar to ImageNet database, if we have bigger dataset and differet from Imagenet, we might need different approach. Here is the guide on how to use Transfer Learning.

**Link of work**: [Transfer Learning Guide](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/img/transferlearning.png)


## Day 44: March 16, 2019

**Today's Progress**: Today, I finished the transfer learning lessons. I used pretrained network which is VGG to classify flowers dataset. First, I loaded the pretrained model, then freeze the parameters so the network acts as a feature extractor. The VGG is trained on ImageNet database which contains 1000 classes so I removed the last layer and replaced it with a new layer to classify the flowers dataset. 

**Link of work**: [Transfer Learning on Flowers Dataset](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/notebooks/transfer_learning_flowers.ipynb)

## Day 45: March 17, 2019

**Today's Progress**: Today I started the second project on the nanodegree which is the Dog Breed Classifier. I finished the first step which is to detect how much percentage humans or dogs are there in the dataset using pretrained face detectors in OpenCV. Next is I will try to detect dogs using pre trained model CNN which is I learned in the previous lessons.

## Day 46: March 18, 2019

**Today's Progress**: Today, I worked on the step two of the project which is to used pretrained model to predict a single image. I still got errors and I'll try to fix it tomorrow.

## Day 47: March 19, 2019

**Today's Progress**: Today I finished making predictions to a single image with a pretrained model VGG. I was stuck at the mismatch size error but I was able to solve it by applying RandomResizeCrop to 224.

## Day 48: March 20, 2019

**Today's Progress**: Next step in the project is to write a Dog detector. VGG was trained using ImageNet which contains 1000 classes, and some of the datasets include dogs and if you look the dataset, it is on index 151 and 268 inclusive. So, I used this model to detect if there is a dog in the image if the prediction index is between 151 and 268.

## Day 49: March 21, 2019

**Today's Progress**: I haven't done the course today because I attended the Tensorflow Dev Summit Extended Manila. The topics discussed was about what's new in TF 2.0,  model interpretability, google colab and some 5-minute talks about RNN for Filipino languages, and image classification for radiology and object detection for cars at night.

## Day 50: March 22, 2019

**Today's Progress**: Today I just created transforms and data loaders for train, valid, and test datasets of dog images. I preprocess the data by aplying some resize and randomcrop transforsm in the dataset.

## Day 51: March 23, 2019

**Today's Progress**

## Day 52: March 24, 2019

**Today's Progress**

## Day 53: March 25, 2019

**Today's Progress**

## Day 54: March 26, 2019

**Today's Progress**

## Day 55: March 27, 2019

**Today's Progress**

## Day 56: March 28, 2019

**Today's Progress**

## Day 57: March 29, 2019

**Today's Progress**


## Day 58: March 30, 2019

**Today's Progress**

## Day 59: March 31, 2019

**Today's Progress**

## Day 60: April 1, 2019

**Today's Progress**

## Day 61: April 2, 2019

**Today's Progress**

## Day 62: April 3, 2019

**Today's Progress**

## Day 63: April 4, 2019

**Today's Progress**











