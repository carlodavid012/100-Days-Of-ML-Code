

<p align="center">
  <img width="550" height="400" src="https://raw.githubusercontent.com/carlodavid012/100-Days-Of-ML-Code/master/img/100-days-of-code-challenge-accepted.jpg">
</p>

This repository contains my progress for the challenge 100DaysOfMLCode, It is a committment to better my understanding of this powerful tool by dedicating at least 1 hour of my time everyday to studying and/or coding machine learning and deep learning for 100 days.



## Day 0 : July 6, 2018

**Today's Progress** : I have setup all the things I needed to complete this challenge. I completed the first part of Introduction to Data Science in Python on Coursera to learn python fundamentals and use advanced Python features, including lambdas, list comprehensions and the numpy library.

**Thoughts** : Hope this will be exciting. The course is really good and it contain lots of good contents. It is good for a beginner.

**Link of work**: [Introduction to Data Science with Python](https://www.coursera.org/learn/python-data-analysis)

## Day 1 : July 7, 2018

**Today's Progress** : Finished the first part of the Machine Learning course on coursera by Andrew Ng up to Linear Regression with One Variable.

**Thoughts** : I learned the application of Linear Regression with one variable to housing price prediction, the notion of a cost function, and the gradient descent method for learning.

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 2 : July 8, 2018

**Today's Progress** : Since Linear Algebra is necessary on learning Machine Learning, I review on basic linear algebra concepts. I finished the Linear Algebra Review on machine learning course on Coursera. 

**Thoughts** : I learned about matrices and vectors, addition and scalar multiplication, matrix-vector and matrix-matrix multiplication, matrix multiplication properties, matrix inverse and matrix transpose operation.

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 3 : July 9, 2018

**Today's Progress** : Learned linear regression using gradient descent and its equations.

**Thoughts** : The concepts are really good. The Math is explained in a practical way.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/Day%203%20-%20linear%20regression%20using%20gradient%20descent/gradient_descent.py) | [How to Do Linear Regression using Gradient Descent](https://www.youtube.com/watch?v=XdM6ER7zTLk)

## Day 4 : July 10, 2018

**Today's Progress** : Learned about Linear Regression with multiple variables, Polynomial Regression, Feature Scaling, and Normal Equation. I  also learned using Octave and did some basic operations, plotting data, control statements, vectorization and finished the first programming assignment.

**Thoughts** : Some concepts are quite difficult at first but I'm able to understand it. I learned how linear regression can be extended to accommodate multiple input features and best practices for implementing linear regression.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/commit/c43391cb2ec8128f548e15dc102357c25b9645c7) | [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 5 : July 11, 2018

**Today's Progress** : Learned about Logistic Regression for classification. I also learned about the notion of classification, the cost function for logistic regression, and the application of logistic regression to multi-class classification.

**Thoughts** : There's a lot of new concepts I learned in this topic. One of it is solving the problem of overfitting using regularization. 

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 6 : July 12, 2018

**Today's Progress** : Watched the Logistic Regression video on youtube by Siraj. I learned about logistic regression using Newton's method for optimization.

**Thoughts** : There's a lot of new terms that I encountered so I need to review them. One of the things I learned is Newton's method usually converges faster than gradient descent when maximizing logistic regression log likelihood.

**Link of work**: [Logistic Regression by Siraj](https://www.youtube.com/watch?v=D8alok2P468&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D&index=4)

## Day 7 : July 13, 2018

**Today's Progress** : Finished the programming assignment for Logistic Regression in the Machine Learning Course on Coursera.

**Thoughts** : The assignment is challenging and difficult for me because I need to write the solution using Octave. The programming exercise tutorials and discussion forums of the course helped me in order to finish it.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/commit/47b70f0fa40c907ff413193013ff0537035663c5)

## Day 8 : July 14, 2018

**Today's Progress** : Learned about how neural networks works and how it learns using backpropagation. 

**Thoughts** : Artificial Neural Networks are pretty amazing.

**Link of work**: [How do Neural Networks work](https://www.youtube.com/watch?v=r9vQhRyrT1M) | [How do Neural Networks Learn](https://www.youtube.com/watch?v=KlKClvoHHoI) | [Neural Networks](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)

## Day 9 : July 15, 2018

**Today's Progress** : I decided to go through Andrew Trask's A Neural Network in 11 lines of Python to really learn how every line worked, and it's been very helpful. 

**Thoughts** : I had to review some matrix math and look up some numpy function calls that he uses, but it was worth it.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/commit/56a10f245fd9e9391f62dc84f151a4ac45168525) | [Neural Networks in Python Part 1](https://iamtrask.github.io/2015/07/12/basic-python-network/)

## Day 10 : July 16, 2018

**Today's Progress** : Watched a video from Siraj Raval on how to make a neural network in python from scratch.

**Thoughts** : Learned more about neural networks and see how it is implemented in python code.

**Link of work**: [Neural Networks from Scratch](https://www.youtube.com/watch?v=vcZub77WvFA)

## Day 11 : July 17, 2018

**Today's Progress** : Continued my machine learning course on coursera which is on Neural Networks. 

**Thoughts** : I learned more theory and intuition about neural networks.

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 12 : July 18, 2018

**Today's Progress** : Learned more about using pandas, which is a Python library for data cleaning and processing. I also learned how to read in data into DataFrame structures, how to query these structures, and the details about such structures are indexed. 

**Thoughts** : Pandas is a very powerful library. It allows you to perform complicated operations efficiently with a small amount of code.

**Link of work**: [Introduction to Data Science with Python](https://www.coursera.org/learn/python-data-analysis)

## Day 13 : July 19, 2018

**Today's Progress** : Continued my Machine Learning course on coursera which is on Week 4 (Neural Networks).

**Thoughts** : Learned about application of neural networks and its examples.

**Link of work**: [Machine Learning Coursera](https://www.coursera.org/learn/machine-learning/)

## Day 14 : July 20, 2018

**Today's Progress** : Today, I decided to brush up my knowledge on Linear Algebra. I watched 3blue1brown video on youtube and finished up to matrix multiplication. 

**Thoughts** : Having a good knowledge on linear algebra is important in learning ML.

**Link of work**: [Essence of Linear Algebra](https://www.youtube.com/watch?v=kjBOesZCoqc&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)

## Day 15 : July 21, 2018

**Today's Progress** : Continued learning linear algebra. Finished up to dot products.

**Thoughts** : Having a good knowledge on linear algebra is important in learning ML.

**Link of work**: [Essence of Linear Algebra](https://www.youtube.com/watch?v=kjBOesZCoqc&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)

## Day 16 : July 22, 2018

**Today's Progress** : Today, I watched the Google machine learning crash course up to validation part.

**Thoughts** : The course has a lot of good content and it is good for beginners.

**Link of work**: [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)

## Day 17 : July 23, 2018

**Today's Progress** : Continued google machine learning crash course up to regularization.

**Thoughts** : Learned about representation,good habits in selecting features, and feature crosses. I also learned about regularization which is used to avoid overfitting.

**Link of work**: [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)

## Day 18 : July 24, 2018

**Today's Progress** : Continued google machine learning crash course up to Introduction to neural nets. I learned about classification, confusion matrix and neural networks.

**Thoughts** : Regularization is important in logistic regression modeling. Accuracy alone doesn't tell the full story when you're working in an imbalanced dataset.

**Link of work**: [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)

## Day 19 : July 25, 2018

**Today's Progress** : Continued google machine learning crash course up to multi-class neural nets.

**Thoughts** : Learned about multi-class neural networks using softmax and one vs all.

**Link of work**: [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/)

## Day 20 : July 26, 2018

**Today's Progress** : Learned about support vector machines which can be used for both classification and regression problems.

**Thoughts** : Support vector machines are great when you have small dataset. The decision of which classifier to choose depends on the dataset and complexity of the problem.

**Link of work**: [Support Vector Machines](https://www.youtube.com/watch?v=g8D5YL6cOSE&index=2&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D)

## Day 21 : July 27, 2018

**Today's Progress** : Learned about k-means clustering which is a type of unsupervised learning algorithm.

**Thoughts** : K-means is usually used when your data is numeric because it doesn't work with categorical features. If you how many classes you want to find, use that as your value for k. If not, you can use the elbow method to find the number of clusters you want to find.

**Link of work**: [K-Means Clustering](https://www.youtube.com/watch?v=9991JlKnFmk&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D&index=6)

## Day 22 : July 28, 2018

**Today's Progress** : Learned about convolutional neural networks which is used for image classification.

**Thoughts** : CNN is quite difficult for me, there's a lot of maths and concepts that I encountered so I need to review them.

**Link of work**: [Convolutional Neural Networks](https://www.youtube.com/watch?v=FTr3n7uBIuE&index=8&list=PL2-dafEMk2A7mu0bSksCGMJEmeddU_H4D)

## Day 23 : November 29, 2018

It's been a long time since my last update,I should have finished it by now :(
So, I decided to continue writing my progress on learning ML and DL. Luckily, I'm selected as one of the recipient of Udacity Pytorch Scholarship Challenge. Two weeks have already passed since the beginning of the challenge, from now on I will start writing my journey on learning Deep Learning. 

## Day 24 : November 30, 2018

**Today's Progress** : Back at it again! Today, I review some concepts of neural networks and study some basics of pytorch and create a simple neural network using pytorch from scratch.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/intro_to_neural_nets.ipynb)

## Day 25 : December 1, 2018

**Today's Progress** : Today, I  build a neural network that can predict the digit in the image using the MNIST dataset. We can build one of these deep networks using only weight matrices as I did in the previous notebook, but in general it's very cumbersome and difficult to implement. PyTorch has a nice module nn that provides a nice way to efficiently build large neural networks.

**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/neural_networks.ipynb)

## Day 26 : December 2, 2018

**Today's Progress** : I continued the notebook on classifying digits using MNIST dataset. So far, i've been using softmax activation function but in general any function can be used as activation function, but it must be non linear, here are some examples of activation function: sigmoid, Tanh(hyperbolic tangent) and ReLU (rectified linear unit). In practice, ReLU is usually used as activation function in hidden layers.


**Link of work**: [Commit](https://github.com/carlodavid012/100-Days-Of-ML-Code/blob/master/neural_networks.ipynb)
